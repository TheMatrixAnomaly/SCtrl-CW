{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/93png3097351dgswjvc0hbtc0000gn/T/ipykernel_31231/3622318156.py:232: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3729.)\n",
      "  return (self.current_state.T @ self.R @ self.current_state).item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Actor Loss: -13.504015922546387\n",
      "Episode 50, Actor Loss: -15.347360610961914\n",
      "Episode 100, Actor Loss: 7.568218231201172\n",
      "Episode 150, Actor Loss: 4.637603759765625\n",
      "Episode 200, Actor Loss: -13.90100383758545\n",
      "Episode 250, Actor Loss: -9.350479125976562\n",
      "Max mean error: 44.35244825049869\n",
      "Average mean error: 16.685590350926848\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.distributions import MultivariateNormal\n",
    "from scipy.integrate import solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###############################################################################\n",
    "# 1) SoftLQR Class\n",
    "#    - Solves the Riccati ODE for the soft LQR problem.\n",
    "#    - Provides a method to compute the optimal value function v*(t, x).\n",
    "###############################################################################\n",
    "class SoftLQR:\n",
    "    def __init__(self, H, M, C, D, R, sigma, time_grid):\n",
    "        \"\"\"\n",
    "        H, M, C, D, R, sigma: cost and dynamics matrices for LQR.\n",
    "        time_grid: a 1D torch.Tensor specifying times from 0 to T (the horizon).\n",
    "        \"\"\"\n",
    "        # Convert the PyTorch tensors to numpy arrays for use in scipy routines.\n",
    "        self.H = H.numpy()\n",
    "        self.M = M.numpy()\n",
    "        self.C = C.numpy()\n",
    "        self.D = D.numpy()\n",
    "        self.R = R.numpy()\n",
    "        self.sigma = sigma.numpy()\n",
    "\n",
    "        # Store the time grid as a numpy array as well.\n",
    "        self.time_grid = time_grid.numpy()\n",
    "\n",
    "        # Will store solutions of the Riccati ODE.\n",
    "        self.solution_finite = None\n",
    "        self.solution_infinite = None\n",
    "        self.solution_history = []\n",
    "        self.stability_results = []\n",
    "\n",
    "        # These are the relaxation parameter (tau) and scale parameter (gamma).\n",
    "        self.tau = None\n",
    "        self.gamma = None\n",
    "\n",
    "    def solve_ricatti_finite(self, T_max, tau=0.5, gamma=1.0):\n",
    "        \"\"\"\n",
    "        Solve the finite-horizon Riccati ODE backward from T_max to 0:\n",
    "          S'(t) + Hᵀ S(t) + S(t) H + C - S(t) M Σ Mᵀ S(t) = 0,\n",
    "        with terminal condition S(T_max) = R.\n",
    "        Stores the solution in self.solution_history for each time,\n",
    "        and self.solution_finite for the final matrix at time 0.\n",
    "        \"\"\"\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.solution_history.clear()\n",
    "\n",
    "        # Right-hand side of the Riccati ODE.\n",
    "        def riccati_rhs(t, S_flat):\n",
    "            S = S_flat.reshape(self.R.shape)\n",
    "            gamma_adjust = tau / (2 * gamma**2)\n",
    "\n",
    "            # Add a small diagonal to avoid singular matrix issues.\n",
    "            inverse_term = np.linalg.inv(self.D + gamma_adjust * np.eye(self.D.shape[0])\n",
    "                                         + 1e-8 * np.eye(self.D.shape[0]))\n",
    "\n",
    "            # - (Hᵀ S + S H + C - S M Σ Mᵀ S).\n",
    "            S_dot = -(self.H.T @ S + S @ self.H + self.C\n",
    "                      - S.T @ self.M @ inverse_term @ self.M.T @ S)\n",
    "            return S_dot.ravel()\n",
    "\n",
    "        # Flatten the terminal condition S(T_max) = R.\n",
    "        S_initial = self.R.ravel()\n",
    "\n",
    "        # Solve backward from T_max to 0 using 'BDF' method.\n",
    "        sol = solve_ivp(\n",
    "            riccati_rhs, [T_max, 0], S_initial, method='BDF',\n",
    "            t_eval=np.linspace(T_max, 0, 500), max_step=10.0,\n",
    "            atol=1e-4, rtol=1e-3\n",
    "        )\n",
    "        if not sol.success:\n",
    "            raise RuntimeError(f\"Riccati ODE Solver failed: {sol.message}\")\n",
    "\n",
    "        # Store the solution at each time point in solution_history.\n",
    "        for i in range(sol.y.shape[1]):\n",
    "            self.solution_history.append(sol.y[:, i].reshape(self.R.shape).astype(np.float32))\n",
    "\n",
    "        # The final matrix at time 0 is sol.y[:, -1].\n",
    "        self.solution_finite = torch.from_numpy(sol.y[:, -1].reshape(self.R.shape).astype(np.float32))\n",
    "\n",
    "    def solve_ricatti_infinite(self, tau=0.5, gamma=1.0):\n",
    "        \"\"\"\n",
    "        Solve the infinite-horizon algebraic Riccati equation using solve_continuous_are.\n",
    "        Stores the result in self.solution_infinite.\n",
    "        \"\"\"\n",
    "        from scipy.linalg import solve_continuous_are, LinAlgError\n",
    "        A = self.H\n",
    "        B = self.M\n",
    "        Q = self.C\n",
    "        gamma_adjust = tau / (2 * gamma**2)\n",
    "\n",
    "        try:\n",
    "            S_ss = solve_continuous_are(A, B, Q, self.D + gamma_adjust * np.eye(self.D.shape[0]))\n",
    "            self.solution_infinite = torch.from_numpy(S_ss.astype(np.float32))\n",
    "        except LinAlgError:\n",
    "            raise RuntimeError(\"Singular matrix encountered during CARE solution.\")\n",
    "\n",
    "    def get_S_at_time(self, t):\n",
    "        \"\"\"\n",
    "        Returns the Riccati solution matrix S(t) at the closest point in the stored solution_history.\n",
    "        \"\"\"\n",
    "        if not self.solution_history:\n",
    "            raise ValueError(\"Solutions have not been computed. Please run solve_ricatti_finite first.\")\n",
    "        closest_time_index = np.argmin(np.abs(self.time_grid - t))\n",
    "        return torch.tensor(self.solution_history[closest_time_index], dtype=torch.float32)\n",
    "\n",
    "    def calculate_trace_integral_simpson(self, start_index):\n",
    "        \"\"\"\n",
    "        Compute the integral of trace(σ σᵀ S(r)) from time_grid[start_index] to the end,\n",
    "        using Simpson's rule. \n",
    "        \"\"\"\n",
    "        trace_integral = 0.0\n",
    "        n = len(self.solution_history)\n",
    "        if (n - start_index) < 3:\n",
    "            return 0.0\n",
    "        dt = self.time_grid[1] - self.time_grid[0]\n",
    "\n",
    "        # We ensure an odd number of points for Simpson's rule.\n",
    "        end_index = n if (n - start_index) % 2 == 1 else n - 1\n",
    "\n",
    "        for i in range(start_index, end_index - 1, 2):\n",
    "            S_i = self.solution_history[i]\n",
    "            S_ip1 = self.solution_history[i+1]\n",
    "            S_ip2 = self.solution_history[i+2]\n",
    "\n",
    "            trace_i = np.trace(self.sigma @ self.sigma.T @ S_i)\n",
    "            trace_ip1 = np.trace(self.sigma @ self.sigma.T @ S_ip1)\n",
    "            trace_ip2 = np.trace(self.sigma @ self.sigma.T @ S_ip2)\n",
    "\n",
    "            trace_integral += (trace_i + 4*trace_ip1 + trace_ip2) * dt / 3\n",
    "        return trace_integral\n",
    "\n",
    "    def calculate_control_problem_value(self, t, x):\n",
    "        \"\"\"\n",
    "        Computes the soft-LQR optimal value function at time t, state x:\n",
    "          v*(t,x) = xᵀ S(t) x + ∫ₜ^T trace(σ σᵀ S(r)) dr + (T - t)*kappa,\n",
    "        where kappa = -τ ln( (τ^(m/2)) / (γ^m) * sqrt(det( (D + τ/(2γ²)I)⁻¹ )) ).\n",
    "        \"\"\"\n",
    "        S_t = self.get_S_at_time(t)  # S(t) from the solution_history\n",
    "        x_np = x.numpy()\n",
    "\n",
    "        # xᵀ S(t) x\n",
    "        quadratic_term = x_np @ (S_t.numpy() @ x_np)\n",
    "\n",
    "        # We compute the constant kappa from tau, gamma, and D.\n",
    "        m = S_t.shape[0]\n",
    "        gamma_adjust = self.tau / (2 * self.gamma**2)\n",
    "        inverse_sigma = np.linalg.inv(self.D + gamma_adjust * np.eye(m))\n",
    "        det_sigma = np.linalg.det(inverse_sigma)\n",
    "\n",
    "        # kappa\n",
    "        C_D_tau_gamma = -self.tau * np.log(\n",
    "            (self.tau**(m/2)) / (self.gamma**m) * math.sqrt(det_sigma)\n",
    "        )\n",
    "\n",
    "        # The integral of trace(σσᵀS(r)) from t to T.\n",
    "        start_index = np.argmin(np.abs(self.time_grid - t))\n",
    "        trace_integral = self.calculate_trace_integral_simpson(start_index)\n",
    "\n",
    "        horizon = self.time_grid[-1] - t\n",
    "        return torch.tensor(quadratic_term + trace_integral + horizon*C_D_tau_gamma, dtype=torch.float32)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) LQREnvironmentWithPolicy\n",
    "#    - Simulates the continuous-time LQR dynamics with Euler-Maruyama steps.\n",
    "#    - Each step returns the new state and the immediate cost.\n",
    "###############################################################################\n",
    "class LQREnvironmentWithPolicy:\n",
    "    def __init__(self, H, M, C, D, R, sigma, gamma, initial_distribution, T, dt):\n",
    "        \"\"\"\n",
    "        H, M, C, D, R, sigma: same LQR matrices.\n",
    "        gamma: scale parameter for the reference distribution (not a learning rate).\n",
    "        initial_distribution: used to sample the initial state.\n",
    "        T, dt: final time and time step for the environment simulation.\n",
    "        \"\"\"\n",
    "        self.H = H\n",
    "        self.M = M\n",
    "        self.C = C\n",
    "        self.D = D\n",
    "        self.R = R\n",
    "        self.sigma = sigma\n",
    "        self.gamma = gamma\n",
    "        self.initial_distribution = initial_distribution\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.device = torch.device('cpu')\n",
    "        self.current_state = None\n",
    "\n",
    "        # N = number of steps in one episode\n",
    "        self.N = int(T / dt)\n",
    "        self.action_dim = M.size(1)\n",
    "\n",
    "    def sample_initial_state(self):\n",
    "        \"\"\"\n",
    "        Sample the initial state X0 from the provided distribution.\n",
    "        \"\"\"\n",
    "        self.current_state = self.initial_distribution.to(self.device)\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        One Euler-Maruyama step:\n",
    "          X_{n+1} = X_n + (H X_n + M a_n) dt + sigma * sqrt(dt) * noise.\n",
    "        Returns (new_state, cost).\n",
    "        \"\"\"\n",
    "        action = action.view(-1, 1)\n",
    "        noise = torch.randn((2,1), dtype=torch.float32, device=self.device) * math.sqrt(self.dt)\n",
    "\n",
    "        current_state_col = self.current_state.unsqueeze(1)\n",
    "        new_state_col = current_state_col + (self.H @ current_state_col + self.M @ action)*self.dt + self.sigma @ noise\n",
    "        new_state = new_state_col.squeeze()\n",
    "\n",
    "        # cost = (xᵀ C x + aᵀ D a)*dt\n",
    "        state_cost = (current_state_col.T @ self.C @ current_state_col).item()\n",
    "        action_cost = (action.T @ self.D @ action).item()\n",
    "        cost = (state_cost + action_cost) * self.dt\n",
    "\n",
    "        self.current_state = new_state\n",
    "        return self.current_state, cost\n",
    "\n",
    "    def observe_terminal_cost(self):\n",
    "        \"\"\"\n",
    "        Return the terminal cost g_T = xᵀ R x at final time T.\n",
    "        \"\"\"\n",
    "        return (self.current_state.T @ self.R @ self.current_state).item()\n",
    "\n",
    "    def f(self, action, t, x):\n",
    "        \"\"\"\n",
    "        The immediate 'physical' cost function f(x,a) = xᵀ C x + aᵀ D a.\n",
    "        (Used in log-prob computations for the \"fixed\" policy.)\n",
    "        \"\"\"\n",
    "        xCx = (x.T @ self.C @ x).item()\n",
    "        aDa = (action.T @ self.D @ action).item()\n",
    "        return xCx + aDa\n",
    "\n",
    "    def gaussian_quadratic_integral(self):\n",
    "        \"\"\"\n",
    "        Compute the normalizing constant for the distribution exp(-f(x,a)).\n",
    "        This is used by the 'fixed_policy_log_prob' for debugging or reference.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            epsilon = 1e-8\n",
    "            adjusted_matrix = torch.eye(self.action_dim)/(2*self.gamma**2) - self.D\n",
    "            adjusted_matrix += torch.eye(self.action_dim)*epsilon\n",
    "            precision_matrix = torch.inverse(adjusted_matrix)\n",
    "            integral_value = torch.sqrt((2*np.pi)**self.action_dim * torch.linalg.det(precision_matrix)).item()\n",
    "            return integral_value\n",
    "        except torch.linalg.LinAlgError as e:\n",
    "            print(\"Matrix inversion failed:\", e)\n",
    "            return float('inf')\n",
    "\n",
    "    def fixed_policy_log_prob(self, action, t, state):\n",
    "        \"\"\"\n",
    "        This is the log of π(a|x) ∝ exp( -f(x,a) ), with some reference normalizing constant.\n",
    "        It's not used in the actor training, but for demonstration if you want a fixed policy.\n",
    "        \"\"\"\n",
    "        f_atx = self.f(action, t, state)\n",
    "        integral_value = self.gaussian_quadratic_integral()\n",
    "        log_denominator = np.log(integral_value)\n",
    "        # log_prob = - f(x,a) - log_denominator\n",
    "        return -f_atx - log_denominator\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 3) GaussianPolicyNN (Actor)\n",
    "#    - A neural network that outputs a Gaussian distribution over actions for each (t, x).\n",
    "###############################################################################\n",
    "class GaussianPolicyNN(nn.Module):\n",
    "    def __init__(self, device=torch.device(\"cpu\"), state_dim=2, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # A small MLP with two hidden layers.\n",
    "        # Input dimension = state_dim + 1, i.e. [t, x1, x2].\n",
    "        self.fc1 = nn.Linear(state_dim + 1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Output heads: mean_head for the mean (2D), and log_std for the diagonal log-variance (2D).\n",
    "        self.mean_head = nn.Linear(hidden_dim, state_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(state_dim))\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        Forward pass: takes time t and state x, returns mean, log_std for the action distribution.\n",
    "        \"\"\"\n",
    "        t = t.to(torch.float32)\n",
    "        x = x.to(torch.float32)\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0)\n",
    "\n",
    "        # Concatenate t and x => shape (1,3) after unsqueeze(0).\n",
    "        input_vec = torch.cat([t, x], dim=0).unsqueeze(0)\n",
    "\n",
    "        # Pass through the MLP.\n",
    "        z = self.activation(self.fc1(input_vec))\n",
    "        z = self.activation(self.fc2(z))\n",
    "\n",
    "        # mean is shape (2,), log_std is shape (2,)\n",
    "        mean = self.mean_head(z).squeeze(0)\n",
    "        return mean, self.log_std\n",
    "\n",
    "    def get_action_distribution(self, t, x):\n",
    "        \"\"\"\n",
    "        Construct a MultivariateNormal with the mean, cov given by the NN outputs.\n",
    "        \"\"\"\n",
    "        mean, log_std = self.forward(t, x)\n",
    "        cov = torch.diag(log_std.exp()**2)\n",
    "        return MultivariateNormal(mean, cov)\n",
    "\n",
    "    def sample_action(self, t, x):\n",
    "        \"\"\"\n",
    "        Sample an action from the current policy at (t, x).\n",
    "        \"\"\"\n",
    "        dist = self.get_action_distribution(t, x)\n",
    "        return dist.sample()\n",
    "\n",
    "    def log_prob(self, t, x, action):\n",
    "        \"\"\"\n",
    "        Return log π_\\theta(a | t, x).\n",
    "        \"\"\"\n",
    "        dist = self.get_action_distribution(t, x)\n",
    "        return dist.log_prob(action)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4) Actor-only training loop\n",
    "#    - Offline style: collect one full episode, then do one gradient update.\n",
    "###############################################################################\n",
    "def offline_actor_algorithm(env, soft_lqr, num_episodes=500, dt=0.005, tau=0.5, actor_lr=1e-3):\n",
    "    \"\"\"\n",
    "    Runs the actor-only policy gradient algorithm using:\n",
    "      advantage_n = cost_n + tau * log_prob_n * dt + (V_{n+1} - V_n) (+ terminal if last).\n",
    "    The policy is updated once per episode.\n",
    "\n",
    "    :param env: LQREnvironmentWithPolicy instance.\n",
    "    :param soft_lqr: SoftLQR instance with a known baseline v*(t,x).\n",
    "    :param num_episodes: how many episodes to train.\n",
    "    :param dt: time step used in the environment (and for log_prob * dt).\n",
    "    :param tau: the relaxation parameter weighting the entropy term.\n",
    "    :param actor_lr: learning rate for the policy network.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    policy_net = GaussianPolicyNN(device=device).to(device)\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=actor_lr)\n",
    "\n",
    "    losses = []  # track the \"actor loss\" each episode\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Sample initial state from the environment\n",
    "        X0 = env.sample_initial_state().to(device)\n",
    "        X = X0.clone()\n",
    "\n",
    "        # Lists to store trajectory info\n",
    "        states, actions, costs, log_probs, times = [], [], [], [], []\n",
    "\n",
    "        # 1) Roll out one full episode\n",
    "        for n in range(env.N):\n",
    "            t_n = n * dt\n",
    "            states.append(X)\n",
    "            times.append(t_n)\n",
    "\n",
    "            # sample action from current policy\n",
    "            a_n = policy_net.sample_action(t=torch.tensor(t_n, dtype=torch.float32, device=device), x=X)\n",
    "            actions.append(a_n)\n",
    "\n",
    "            # step environment\n",
    "            X_next, cost_n = env.step(a_n)\n",
    "            costs.append(cost_n)\n",
    "\n",
    "            # store log_prob for the gradient update\n",
    "            lp = policy_net.log_prob(t=torch.tensor(t_n, dtype=torch.float32, device=device),\n",
    "                                     x=states[-1], action=a_n)\n",
    "            log_probs.append(lp)\n",
    "\n",
    "            X = X_next.clone()\n",
    "\n",
    "        # terminal cost g_T\n",
    "        g_T = env.observe_terminal_cost()\n",
    "\n",
    "        # 2) Compute the policy gradient objective => \"actor_loss\"\n",
    "        actor_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "        for n in range(env.N):\n",
    "            t_n = times[n]\n",
    "            X_n = states[n]\n",
    "\n",
    "            # baseline advantage uses V_next - V_n\n",
    "            if n < env.N - 1:\n",
    "                t_next = times[n+1]\n",
    "                X_next = states[n+1]\n",
    "                V_next = soft_lqr.calculate_control_problem_value(t_next, X_next).to(device)\n",
    "            else:\n",
    "                V_next = torch.tensor(0.0, device=device)\n",
    "\n",
    "            V_n = soft_lqr.calculate_control_problem_value(t_n, X_n).to(device)\n",
    "\n",
    "            # advantage = cost + tau*log_prob*dt + (V_next - V_n)\n",
    "            adv = costs[n] + tau * log_probs[n] * dt + (V_next - V_n)\n",
    "\n",
    "            # if last step, add terminal cost\n",
    "            if n == env.N - 1:\n",
    "                adv += g_T\n",
    "\n",
    "            # Summation of - log_prob * advantage => negative because we want to maximize\n",
    "            actor_loss = actor_loss - log_probs[n] * adv\n",
    "\n",
    "        # 3) One gradient update\n",
    "        optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(actor_loss.item())\n",
    "\n",
    "        # Print progress every 50 episodes\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}, Actor Loss: {actor_loss.item()}\")\n",
    "\n",
    "    return policy_net\n",
    "\n",
    "###############################################################################\n",
    "# 5) Rollout and evaluation\n",
    "###############################################################################\n",
    "def rollout_cost_from_t_x(H, M, C, D, R, sigma, dt, t_start, T_final, x_init, policy_net, steps_noise_seed=None):\n",
    "    \"\"\"\n",
    "    Simulate from time t_start to T_final with Euler-Maruyama steps, using the policy_net\n",
    "    to choose actions. Accumulate cost (xᵀCx + aᵀDa)*dt, plus terminal cost xᵀ R x.\n",
    "    Returns the total cost from that sub-trajectory.\n",
    "    \"\"\"\n",
    "    if steps_noise_seed is not None:\n",
    "        torch.manual_seed(steps_noise_seed)\n",
    "    n_steps = int((T_final - t_start) / dt)\n",
    "    x = x_init.clone()\n",
    "    total_cost = 0.0\n",
    "\n",
    "    for n in range(n_steps):\n",
    "        t_n = t_start + n*dt\n",
    "        a = policy_net.sample_action(t=torch.tensor(t_n, dtype=torch.float32), x=x)\n",
    "\n",
    "        # cost = (xᵀCx + aᵀDa)*dt\n",
    "        state_cost = (x.unsqueeze(0) @ C @ x.unsqueeze(1)).item()\n",
    "        action_cost = (a.unsqueeze(0) @ D @ a.unsqueeze(1)).item()\n",
    "        total_cost += (state_cost + action_cost) * dt\n",
    "\n",
    "        # next state\n",
    "        noise = torch.randn((2,), dtype=torch.float32) * math.sqrt(dt)\n",
    "        x = x + (H @ x + M @ a)*dt + sigma @ noise\n",
    "\n",
    "    # terminal cost\n",
    "    terminal_cost = (x.unsqueeze(0) @ R @ x.unsqueeze(1)).item()\n",
    "    total_cost += terminal_cost\n",
    "    return total_cost\n",
    "\n",
    "###############################################################################\n",
    "# 6) Compare Learned vs. Optimal Controls\n",
    "###############################################################################\n",
    "def optimal_control_from_softlqr(soft_lqr, t, x, tau=0.2, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Computes the optimal control distribution from the SoftLQR solution:\n",
    "      mean_opt = - Σ Mᵀ S(t) x,\n",
    "      Σ_opt = (D + (τ/(2γ²))I)⁻¹,\n",
    "      cov_opt = τ * (Σ_opt)⁻¹.\n",
    "    \"\"\"\n",
    "    S_t = soft_lqr.get_S_at_time(t).numpy()\n",
    "    m = soft_lqr.D.shape[0]\n",
    "\n",
    "    # Σ_opt\n",
    "    Sigma = np.linalg.inv(soft_lqr.D + (tau/(2*gamma**2)) * np.eye(m))\n",
    "\n",
    "    # mean = - Σ_opt Mᵀ S(t) x\n",
    "    mean_opt = - Sigma @ soft_lqr.M.T @ S_t @ x.numpy()\n",
    "\n",
    "    # covariance = τ Σ_opt⁻¹\n",
    "    cov_opt = tau * np.linalg.inv(Sigma)\n",
    "    return mean_opt, cov_opt\n",
    "\n",
    "def compare_controls(policy_net, soft_lqr,\n",
    "                     times=[0.0, 1/6, 2/6, 0.5],\n",
    "                     x_min=-3.0, x_max=3.0, grid_size=7,\n",
    "                     tau=0.5, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compare the learned policy's mean & covariance with the theoretical optimal control\n",
    "    at a grid of (t,x). We measure the average Euclidean error for means and average\n",
    "    Frobenius norm error for covariances, and print them.\n",
    "    \"\"\"\n",
    "    x_vals = np.linspace(x_min, x_max, grid_size)\n",
    "    mean_errors = []\n",
    "    cov_errors = []\n",
    "\n",
    "    for t_val in times:\n",
    "        for x1 in x_vals:\n",
    "            for x2 in x_vals:\n",
    "                x_tensor = torch.tensor([x1, x2], dtype=torch.float32)\n",
    "\n",
    "                # Learned distribution\n",
    "                dist_learned = policy_net.get_action_distribution(torch.tensor(t_val, dtype=torch.float32), x_tensor)\n",
    "                learned_mean = dist_learned.mean.detach().numpy()\n",
    "                learned_cov = dist_learned.covariance_matrix.detach().numpy()\n",
    "\n",
    "                # Optimal distribution\n",
    "                optimal_mean, optimal_cov = optimal_control_from_softlqr(soft_lqr, t_val, x_tensor, tau, gamma)\n",
    "\n",
    "                # Euclidean error for means\n",
    "                mean_err = np.linalg.norm(learned_mean - optimal_mean)\n",
    "                # Frobenius norm for covariance error\n",
    "                cov_err = np.linalg.norm(learned_cov - optimal_cov)\n",
    "\n",
    "                mean_errors.append(mean_err)\n",
    "                cov_errors.append(cov_err)\n",
    "\n",
    "    print(\"Max mean error:\", np.max(mean_errors))\n",
    "    print(\"Average mean error:\", np.mean(mean_errors))\n",
    "\n",
    "    return np.mean(mean_errors), np.mean(cov_errors)\n",
    "\n",
    "###############################################################################\n",
    "# 7) Main Script\n",
    "###############################################################################\n",
    "\n",
    "# 1) Set up LQR problem data.\n",
    "H = torch.tensor([[1.0, 1.0],\n",
    "                    [0.0, 1.0]], dtype=torch.float32) * 0.5\n",
    "M = torch.tensor([[1.0, 0.0],\n",
    "                    [0.0, 1.0]], dtype=torch.float32)\n",
    "C = torch.tensor([[1.0, 0.1],\n",
    "                    [0.1, 1.0]], dtype=torch.float32)\n",
    "D = torch.tensor([[1.0, 0.0],\n",
    "                    [0.0, 1.0]], dtype=torch.float32)\n",
    "R = torch.tensor([[1.0, 0.3],\n",
    "                    [0.3, 1.0]], dtype=torch.float32) * 10.0\n",
    "sigma = torch.eye(2, dtype=torch.float32) * 0.5\n",
    "\n",
    "# Final time T = 0.5, discretized into 100 points for the Riccati solution.\n",
    "T = 0.5\n",
    "N_points = 100\n",
    "time_grid = torch.linspace(0, T, N_points)\n",
    "\n",
    "# 2) Build a SoftLQR instance and solve the Riccati ODE.\n",
    "soft_lqr = SoftLQR(H, M, C, D, R, sigma, time_grid)\n",
    "soft_lqr.solve_ricatti_finite(T_max=T, tau=0.5, gamma=1.0)\n",
    "\n",
    "# 3) Build environment with the same T, dt=0.005\n",
    "initial_distribution = torch.FloatTensor(2).uniform_(-2.0, 2.0)\n",
    "env_policy = LQREnvironmentWithPolicy(\n",
    "    H, M, C, D, R, sigma, gamma=1.0,\n",
    "    initial_distribution=initial_distribution,\n",
    "    T=T, dt=0.005\n",
    ")\n",
    "\n",
    "# 4) Train the actor-only policy for 300 episodes, with tau=0.5 in the advantage\n",
    "#    and a very small learning rate actor_lr=1e-6 to keep updates stable.\n",
    "policy_net = offline_actor_algorithm(\n",
    "    env_policy, soft_lqr,\n",
    "    num_episodes=300, dt=0.005,\n",
    "    tau=0.5, actor_lr=1e-6\n",
    ")\n",
    "\n",
    "# 5) Compare the learned control distribution with the optimal control distribution.\n",
    "avg_mean_err, avg_cov_err = compare_controls(\n",
    "    policy_net, soft_lqr,\n",
    "    times=[0.0, 1/6, 2/6, 0.5],\n",
    "    x_min=-3.0, x_max=3.0, grid_size=7,\n",
    "    tau=0.5, gamma=1.0\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
