{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0: Critic Loss: 176385.9531, Actor Loss: 148.0489\n",
      "Episode 50: Critic Loss: 725275.0000, Actor Loss: 320.8912\n",
      "Episode 100: Critic Loss: 438044.0000, Actor Loss: 165.0351\n",
      "Episode 150: Critic Loss: 615037.4375, Actor Loss: 184.6769\n",
      "Episode 200: Critic Loss: 179211.9531, Actor Loss: 96.0362\n",
      "Episode 250: Critic Loss: 236634.4688, Actor Loss: 183.7198\n",
      "Episode 300: Critic Loss: 209090.7969, Actor Loss: 108.9466\n",
      "Episode 350: Critic Loss: 373778.8125, Actor Loss: 140.7334\n",
      "Episode 400: Critic Loss: 502283.3438, Actor Loss: 204.0352\n",
      "Episode 450: Critic Loss: 1042841.4375, Actor Loss: 305.3223\n",
      "Episode 500: Critic Loss: 658376.8750, Actor Loss: 257.3591\n",
      "Actor-Critic training completed.\n",
      "Max value function error: 107.92829656600952\n",
      "Average value function error: 45.151035868045355\n",
      "Max control mean error: 29.744786486896096\n",
      "Average control mean error: 13.540088084515704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13.540088084515704, 0.5303600119706506)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.distributions import MultivariateNormal\n",
    "from scipy.integrate import solve_ivp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###############################################################################\n",
    "# 1) SoftLQR Class\n",
    "#    - Solves the Riccati ODE for the soft LQR problem.\n",
    "#    - Provides a method to compute the optimal value function v*(t, x) and the optimal control.\n",
    "###############################################################################\n",
    "class SoftLQR:\n",
    "    def __init__(self, H, M, C, D, R, sigma, time_grid):\n",
    "        # Convert PyTorch tensors to numpy arrays.\n",
    "        self.H = H.numpy()\n",
    "        self.M = M.numpy()\n",
    "        self.C = C.numpy()\n",
    "        self.D = D.numpy()\n",
    "        self.R = R.numpy()\n",
    "        self.sigma = sigma.numpy()\n",
    "        self.time_grid = time_grid.numpy()  # time grid covering [0, T]\n",
    "        self.solution_finite = None\n",
    "        self.solution_infinite = None\n",
    "        self.solution_history = []  # Stores S(t) at each time point.\n",
    "        self.stability_results = []\n",
    "        self.tau = None\n",
    "        self.gamma = None\n",
    "\n",
    "    def solve_ricatti_finite(self, T_max, tau=0.5, gamma=1.0):\n",
    "        \"\"\"\n",
    "        Solves the Riccati ODE backward from T_max to 0 with S(T_max)=R.\n",
    "        Stores the solution history.\n",
    "        \"\"\"\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.solution_history.clear()\n",
    "\n",
    "        def riccati_rhs(t, S_flat):\n",
    "            S = S_flat.reshape(self.R.shape)\n",
    "            gamma_adjust = tau / (2 * gamma**2)\n",
    "            inverse_term = np.linalg.inv(self.D + gamma_adjust * np.eye(self.D.shape[0]) + 1e-8 * np.eye(self.D.shape[0]))\n",
    "            S_dot = -(self.H.T @ S + S @ self.H + self.C - S.T @ self.M @ inverse_term @ self.M.T @ S)\n",
    "            return S_dot.ravel()\n",
    "\n",
    "        S_initial = self.R.ravel()\n",
    "        sol = solve_ivp(riccati_rhs, [T_max, 0], S_initial, method='BDF',\n",
    "                        t_eval=np.linspace(T_max, 0, 500),\n",
    "                        max_step=10.0, atol=1e-4, rtol=1e-3)\n",
    "        if not sol.success:\n",
    "            raise RuntimeError(f\"Riccati ODE Solver failed: {sol.message}\")\n",
    "\n",
    "        for i in range(sol.y.shape[1]):\n",
    "            self.solution_history.append(sol.y[:, i].reshape(self.R.shape).astype(np.float32))\n",
    "        self.solution_finite = torch.from_numpy(sol.y[:, -1].reshape(self.R.shape).astype(np.float32))\n",
    "\n",
    "    def solve_ricatti_infinite(self, tau=0.5, gamma=1.0):\n",
    "        from scipy.linalg import solve_continuous_are, LinAlgError\n",
    "        A = self.H; B = self.M; Q = self.C\n",
    "        gamma_adjust = tau / (2 * gamma**2)\n",
    "        try:\n",
    "            S_ss = solve_continuous_are(A, B, Q, self.D + gamma_adjust * np.eye(self.D.shape[0]))\n",
    "            self.solution_infinite = torch.from_numpy(S_ss.astype(np.float32))\n",
    "        except LinAlgError:\n",
    "            raise RuntimeError(\"Singular matrix encountered during CARE solution.\")\n",
    "\n",
    "    def get_S_at_time(self, t):\n",
    "        \"\"\"\n",
    "        Returns S(t) (as a torch tensor) at the closest time point in solution_history.\n",
    "        \"\"\"\n",
    "        if not self.solution_history:\n",
    "            raise ValueError(\"Run solve_ricatti_finite first.\")\n",
    "        closest_time_index = np.argmin(np.abs(self.time_grid - t))\n",
    "        return torch.tensor(self.solution_history[closest_time_index], dtype=torch.float32)\n",
    "\n",
    "    def calculate_trace_integral_simpson(self, start_index):\n",
    "        \"\"\"\n",
    "        Approximates ∫_{t}^{T} tr(σσᵀS(r))dr using Simpson's rule.\n",
    "        \"\"\"\n",
    "        trace_integral = 0.0\n",
    "        n = len(self.solution_history)\n",
    "        if (n - start_index) < 3:\n",
    "            return 0.0\n",
    "        dt = self.time_grid[1] - self.time_grid[0]\n",
    "        end_index = n if (n - start_index) % 2 == 1 else n - 1\n",
    "        for i in range(start_index, end_index - 1, 2):\n",
    "            S_i = self.solution_history[i]\n",
    "            S_ip1 = self.solution_history[i+1]\n",
    "            S_ip2 = self.solution_history[i+2]\n",
    "            trace_i = np.trace(self.sigma @ self.sigma.T @ S_i)\n",
    "            trace_ip1 = np.trace(self.sigma @ self.sigma.T @ S_ip1)\n",
    "            trace_ip2 = np.trace(self.sigma @ self.sigma.T @ S_ip2)\n",
    "            trace_integral += (trace_i + 4*trace_ip1 + trace_ip2) * dt / 3\n",
    "        return trace_integral\n",
    "\n",
    "    def calculate_control_problem_value(self, t, x):\n",
    "        \"\"\"\n",
    "        Computes the optimal value:\n",
    "          v*(t,x) = xᵀS(t)x + ∫ₜ^T tr(σσᵀS(r))dr + (T-t)*kappa,\n",
    "        where kappa = -τ ln( (τ^(m/2))/(γ^m)*sqrt(det((D+τ/(2γ²)I)⁻¹) ).\n",
    "        \"\"\"\n",
    "        S_t = self.get_S_at_time(t)\n",
    "        x_np = x.numpy()\n",
    "        quadratic_term = x_np @ (S_t.numpy() @ x_np)\n",
    "        m = S_t.shape[0]\n",
    "        gamma_adjust = self.tau / (2 * self.gamma**2)\n",
    "        inverse_sigma = np.linalg.inv(self.D + gamma_adjust * np.eye(m))\n",
    "        det_sigma = np.linalg.det(inverse_sigma)\n",
    "        kappa = -self.tau * np.log((self.tau**(m/2))/(self.gamma**m) * math.sqrt(det_sigma))\n",
    "        start_index = np.argmin(np.abs(self.time_grid - t))\n",
    "        trace_integral = self.calculate_trace_integral_simpson(start_index)\n",
    "        horizon = self.time_grid[-1] - t\n",
    "        return torch.tensor(quadratic_term + trace_integral + horizon * kappa, dtype=torch.float32)\n",
    "\n",
    "    def optimal_control(self, t, x, tau=0.5, gamma=1.0):\n",
    "        \"\"\"\n",
    "        Computes the optimal control distribution parameters from the Riccati solution:\n",
    "         μ* = -Σ_opt Mᵀ S(t)x, with Σ_opt = (D + (τ/(2γ²)) I)⁻¹,\n",
    "         and covariance = τ * (Σ_opt)⁻¹.\n",
    "        \"\"\"\n",
    "        S_t = self.get_S_at_time(t).numpy()\n",
    "        m = self.D.shape[0]\n",
    "        Sigma = np.linalg.inv(self.D + (tau/(2*gamma**2))*np.eye(m))\n",
    "        mean_opt = -Sigma @ self.M.T @ S_t @ x.numpy()\n",
    "        cov_opt = tau * np.linalg.inv(Sigma)\n",
    "        return mean_opt, cov_opt\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 2) LQREnvironmentWithPolicy\n",
    "#    - Simulates the continuous-time LQR dynamics with Euler-Maruyama steps.\n",
    "#    - Each step returns the new state and the immediate cost.\n",
    "###############################################################################\n",
    "class LQREnvironmentWithPolicy:\n",
    "    def __init__(self, H, M, C, D, R, sigma, gamma, initial_distribution, T, dt):\n",
    "        \"\"\"\n",
    "        H, M, C, D, R, sigma: same LQR matrices.\n",
    "        gamma: scale parameter for the reference distribution (not a learning rate).\n",
    "        initial_distribution: used to sample the initial state.\n",
    "        T, dt: final time and time step for the environment simulation.\n",
    "        \"\"\"\n",
    "        self.H = H\n",
    "        self.M = M\n",
    "        self.C = C\n",
    "        self.D = D\n",
    "        self.R = R\n",
    "        self.sigma = sigma\n",
    "        self.gamma = gamma\n",
    "        self.initial_distribution = initial_distribution\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.device = torch.device('cpu')\n",
    "        self.current_state = None\n",
    "\n",
    "        # N = number of steps in one episode\n",
    "        self.N = int(T / dt)\n",
    "        self.action_dim = M.size(1)\n",
    "\n",
    "    def sample_initial_state(self):\n",
    "        \"\"\"\n",
    "        Sample the initial state X0 from the provided distribution.\n",
    "        \"\"\"\n",
    "        self.current_state = self.initial_distribution.to(self.device)\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        One Euler-Maruyama step:\n",
    "          X_{n+1} = X_n + (H X_n + M a_n) dt + sigma * sqrt(dt) * noise.\n",
    "        Returns (new_state, cost).\n",
    "        \"\"\"\n",
    "        action = action.view(-1, 1)\n",
    "        noise = torch.randn((2,1), dtype=torch.float32, device=self.device) * math.sqrt(self.dt)\n",
    "\n",
    "        current_state_col = self.current_state.unsqueeze(1)\n",
    "        new_state_col = current_state_col + (self.H @ current_state_col + self.M @ action)*self.dt + self.sigma @ noise\n",
    "        new_state = new_state_col.squeeze()\n",
    "\n",
    "        # cost = (xᵀ C x + aᵀ D a)*dt\n",
    "        state_cost = (current_state_col.T @ self.C @ current_state_col).item()\n",
    "        action_cost = (action.T @ self.D @ action).item()\n",
    "        cost = (state_cost + action_cost) * self.dt\n",
    "\n",
    "        self.current_state = new_state\n",
    "        return self.current_state, cost\n",
    "\n",
    "    def observe_terminal_cost(self):\n",
    "        \"\"\"\n",
    "        Return the terminal cost g_T = xᵀ R x at final time T.\n",
    "        \"\"\"\n",
    "        return (self.current_state.T @ self.R @ self.current_state).item()\n",
    "\n",
    "    def f(self, action, t, x):\n",
    "        \"\"\"\n",
    "        The immediate 'physical' cost function f(x,a) = xᵀ C x + aᵀ D a.\n",
    "        (Used in log-prob computations for the \"fixed\" policy.)\n",
    "        \"\"\"\n",
    "        xCx = (x.T @ self.C @ x).item()\n",
    "        aDa = (action.T @ self.D @ action).item()\n",
    "        return xCx + aDa\n",
    "\n",
    "    def gaussian_quadratic_integral(self):\n",
    "        \"\"\"\n",
    "        Compute the normalizing constant for the distribution exp(-f(x,a)).\n",
    "        This is used by the 'fixed_policy_log_prob' for debugging or reference.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            epsilon = 1e-8\n",
    "            adjusted_matrix = torch.eye(self.action_dim)/(2*self.gamma**2) - self.D\n",
    "            adjusted_matrix += torch.eye(self.action_dim)*epsilon\n",
    "            precision_matrix = torch.inverse(adjusted_matrix)\n",
    "            integral_value = torch.sqrt((2*np.pi)**self.action_dim * torch.linalg.det(precision_matrix)).item()\n",
    "            return integral_value\n",
    "        except torch.linalg.LinAlgError as e:\n",
    "            print(\"Matrix inversion failed:\", e)\n",
    "            return float('inf')\n",
    "\n",
    "    def fixed_policy_log_prob(self, action, t, state):\n",
    "        \"\"\"\n",
    "        This is the log of π(a|x) ∝ exp( -f(x,a) ), with some reference normalizing constant.\n",
    "        It's not used in the actor training, but for demonstration if you want a fixed policy.\n",
    "        \"\"\"\n",
    "        f_atx = self.f(action, t, state)\n",
    "        integral_value = self.gaussian_quadratic_integral()\n",
    "        log_denominator = np.log(integral_value)\n",
    "        # log_prob = - f(x,a) - log_denominator\n",
    "        return -f_atx - log_denominator\n",
    "    \n",
    "\n",
    "###############################################################################\n",
    "# 3) Critic Network (Value Function Approximator)\n",
    "###############################################################################\n",
    "class OnlyLinearValueNN(nn.Module):\n",
    "    def __init__(self, device=torch.device(\"cpu\")):\n",
    "        \"\"\"\n",
    "        A simple critic network that approximates the value function:\n",
    "          V(t,x) = xᵀ S(t) x + offset(t),\n",
    "        with S(t) a positive semidefinite matrix output and offset a scalar.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.hidden_layer_width = 256\n",
    "\n",
    "        self.hidden_layer = nn.Linear(1, self.hidden_layer_width, device=device)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.matrix_layer = nn.Linear(self.hidden_layer_width, 4, device=device)\n",
    "        self.offset_layer = nn.Linear(self.hidden_layer_width, 1, device=device)\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        Input: t (shape (1,)) \n",
    "        Returns: a 2x2 symmetric matrix S(t) and a scalar offset.\n",
    "        \"\"\"\n",
    "        x = self.activation(self.hidden_layer(t))\n",
    "        matrix_elements = self.matrix_layer(x)  # shape (1,4)\n",
    "        matrix = matrix_elements.view(-1, 2, 2)\n",
    "        S_out = torch.bmm(matrix, matrix.transpose(1,2)) + 1e-3*torch.eye(2).to(self.device)\n",
    "        offset = self.offset_layer(x)\n",
    "        return S_out, offset\n",
    "\n",
    "    def predict_value(self, t, x):\n",
    "        \"\"\"\n",
    "        Returns V(t,x) = xᵀ S(t) x + offset.\n",
    "        \"\"\"\n",
    "        if not isinstance(t, torch.Tensor):\n",
    "            t = torch.tensor([t], dtype=torch.float32, device=self.device)\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0)\n",
    "        S, offset = self.forward(t)\n",
    "        S = S.squeeze(0)\n",
    "        offset = offset.squeeze()\n",
    "        x_col = x.view(-1,1)\n",
    "        return (x_col.t() @ S @ x_col).squeeze() + offset\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 4) Actor Network (Gaussian Policy)\n",
    "###############################################################################\n",
    "class GaussianPolicyNN(nn.Module):\n",
    "    def __init__(self, device=torch.device(\"cpu\"), state_dim=2, hidden_dim=256):\n",
    "        \"\"\"\n",
    "        A neural network that outputs a Gaussian distribution over actions given (t,x).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.fc1 = nn.Linear(state_dim+1, hidden_dim, device=device)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim, device=device)\n",
    "        self.mean_head = nn.Linear(hidden_dim, state_dim, device=device)\n",
    "        self.log_std = nn.Parameter(torch.zeros(state_dim))\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        \"\"\"\n",
    "        Input: t (shape (1,)) and x (shape (2,))\n",
    "        Output: mean (2,) and log_std (2,)\n",
    "        \"\"\"\n",
    "        t = t.to(torch.float32)\n",
    "        x = x.to(torch.float32)\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0)\n",
    "        inp = torch.cat([t, x], dim=0).unsqueeze(0)  # shape (1,3)\n",
    "        z = self.activation(self.fc1(inp))\n",
    "        z = self.activation(self.fc2(z))\n",
    "        mean = self.mean_head(z).squeeze(0)\n",
    "        return mean, self.log_std\n",
    "\n",
    "    def get_action_distribution(self, t, x):\n",
    "        mean, log_std = self.forward(t, x)\n",
    "        cov = torch.diag(log_std.exp()**2)\n",
    "        return MultivariateNormal(mean, cov)\n",
    "\n",
    "    def sample_action(self, t, x):\n",
    "        return self.get_action_distribution(t, x).sample()\n",
    "\n",
    "    def log_prob(self, t, x, action):\n",
    "        return self.get_action_distribution(t, x).log_prob(action)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 5) Actor-Critic Training Algorithm\n",
    "###############################################################################\n",
    "def actor_critic_algorithm(env, num_episodes=500, dt=0.005, tau=0.5, actor_lr=1e-3, critic_lr=1e-3):\n",
    "    \"\"\"\n",
    "    Offline actor-critic algorithm:\n",
    "      - Critic: learns V(t,x) by minimizing MSE between its prediction and the Monte Carlo return.\n",
    "      - Actor: updated using policy gradient with advantage computed as:\n",
    "            A_n = f_n + tau*log(pi(a_n|t_n,x_n))*dt + (V(t_{n+1},x_{n+1}) - V(t_n,x_n))\n",
    "        plus terminal cost.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    # Initialize networks\n",
    "    critic_net = OnlyLinearValueNN(device=device).to(device)\n",
    "    actor_net  = GaussianPolicyNN(device=device).to(device)\n",
    "\n",
    "    critic_optimizer = optim.Adam(critic_net.parameters(), lr=critic_lr)\n",
    "    actor_optimizer  = optim.Adam(actor_net.parameters(), lr=actor_lr)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # 1) Roll out an episode\n",
    "        X0 = env.sample_initial_state().to(device)\n",
    "        X = X0.clone()\n",
    "        states, actions, costs, log_probs, times = [], [], [], [], []\n",
    "\n",
    "        for n in range(env.N):\n",
    "            t_n = n * dt\n",
    "            states.append(X)\n",
    "            times.append(t_n)\n",
    "            a_n = actor_net.sample_action(torch.tensor(t_n, dtype=torch.float32, device=device), X)\n",
    "            actions.append(a_n)\n",
    "            X_next, cost_n = env.step(a_n)\n",
    "            costs.append(cost_n)\n",
    "            lp_n = actor_net.log_prob(torch.tensor(t_n, dtype=torch.float32, device=device), states[-1], a_n)\n",
    "            log_probs.append(lp_n)\n",
    "            X = X_next.clone()\n",
    "\n",
    "        g_T = env.observe_terminal_cost()\n",
    "\n",
    "        # 2) Compute Monte Carlo return for critic update.\n",
    "        returns = []\n",
    "        G = g_T\n",
    "        for cost in reversed(costs):\n",
    "            G = cost + G  # simple sum over episode (could use discounting if needed)\n",
    "            returns.insert(0, G)\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=device)\n",
    "\n",
    "        critic_loss = 0.0\n",
    "        for n in range(env.N):\n",
    "            t_n = times[n]\n",
    "            V_pred = critic_net.predict_value(torch.tensor(t_n, dtype=torch.float32, device=device), states[n])\n",
    "            critic_loss += (V_pred - returns[n])**2\n",
    "\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "\n",
    "        # 3) Actor update using advantage computed with critic.\n",
    "        actor_loss = 0.0\n",
    "        for n in range(env.N):\n",
    "            t_n = times[n]\n",
    "            if n < env.N - 1:\n",
    "                V_next = critic_net.predict_value(torch.tensor(times[n+1], dtype=torch.float32, device=device), states[n+1])\n",
    "            else:\n",
    "                V_next = torch.tensor(0.0, device=device)\n",
    "            V_n = critic_net.predict_value(torch.tensor(t_n, dtype=torch.float32, device=device), states[n])\n",
    "            advantage = costs[n] + tau * log_probs[n] * dt + (V_next - V_n)\n",
    "            if n == env.N - 1:\n",
    "                advantage += g_T\n",
    "            actor_loss = actor_loss - log_probs[n] * advantage\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}: Critic Loss: {critic_loss.item():.4f}, Actor Loss: {actor_loss.item():.4f}\")\n",
    "\n",
    "    return actor_net, critic_net\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 6) Comparison Functions\n",
    "###############################################################################\n",
    "def optimal_control_from_softlqr(soft_lqr, t, x, tau=0.5, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Computes the optimal control distribution from the Riccati solution:\n",
    "      μ* = -Σ_opt Mᵀ S(t)x, with Σ_opt = (D + (τ/(2γ²))I)⁻¹,\n",
    "      and covariance = τ * (Σ_opt)⁻¹.\n",
    "    \"\"\"\n",
    "    S_t = soft_lqr.get_S_at_time(t).numpy()\n",
    "    m = soft_lqr.D.shape[0]\n",
    "    Sigma = np.linalg.inv(soft_lqr.D + (tau/(2*gamma**2)) * np.eye(m))\n",
    "    mean_opt = -Sigma @ soft_lqr.M.T @ S_t @ x.numpy()\n",
    "    cov_opt = tau * np.linalg.inv(Sigma)\n",
    "    return mean_opt, cov_opt\n",
    "\n",
    "def compare_value_functions(critic_net, soft_lqr, times=[0.0, 0.25, 0.5], x_vals=[-2.0, 0.0, 2.0]):\n",
    "    \"\"\"\n",
    "    Compare the critic's learned value V(t,x) with the optimal value v*(t,x) from SoftLQR.\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for t in times:\n",
    "        for x1 in x_vals:\n",
    "            for x2 in x_vals:\n",
    "                x = torch.tensor([x1, x2], dtype=torch.float32)\n",
    "                V_pred = critic_net.predict_value(torch.tensor([t], dtype=torch.float32), x).item()\n",
    "                V_opt = soft_lqr.calculate_control_problem_value(t, x).item()\n",
    "                err = abs(V_pred - V_opt)\n",
    "                errors.append(err)\n",
    "    avg_err = np.mean(errors)\n",
    "\n",
    "    print(\"Max value function error:\", np.max(errors))\n",
    "    print(\"Average value function error:\", avg_err)\n",
    "    return avg_err\n",
    "\n",
    "def compare_control_distributions(actor_net, soft_lqr, times=[0.0, 0.25, 0.5], x_vals=[-2.0, 0.0, 2.0], tau=0.5, gamma=1.0):\n",
    "    \"\"\"\n",
    "    Compare the learned control distribution from the actor with the optimal control\n",
    "    computed from SoftLQR.\n",
    "    \"\"\"\n",
    "    mean_errors = []\n",
    "    cov_errors = []\n",
    "    for t in times:\n",
    "        for x1 in x_vals:\n",
    "            for x2 in x_vals:\n",
    "                x = torch.tensor([x1, x2], dtype=torch.float32)\n",
    "                # Learned distribution:\n",
    "                dist_learned = actor_net.get_action_distribution(torch.tensor(t, dtype=torch.float32), x)\n",
    "                learned_mean = dist_learned.mean.detach().numpy()\n",
    "                learned_cov = dist_learned.covariance_matrix.detach().numpy()\n",
    "\n",
    "                # Optimal distribution:\n",
    "                optimal_mean, optimal_cov = optimal_control_from_softlqr(soft_lqr, t, x, tau, gamma)\n",
    "                mean_err = np.linalg.norm(learned_mean - optimal_mean)\n",
    "                cov_err = np.linalg.norm(learned_cov - optimal_cov)\n",
    "                mean_errors.append(mean_err)\n",
    "                cov_errors.append(cov_err)\n",
    "\n",
    "    print(\"Max control mean error:\", np.max(mean_errors))\n",
    "    print(\"Average control mean error:\", np.mean(mean_errors))\n",
    "\n",
    "    return np.mean(mean_errors), np.mean(cov_errors)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# 7) Main Script: Setup, Train, and Compare\n",
    "###############################################################################\n",
    "\n",
    "# LQR problem data\n",
    "H = torch.tensor([[1.0, 1.0],\n",
    "                    [0.0, 1.0]], dtype=torch.float32) * 0.5\n",
    "M = torch.tensor([[1.0, 0.0],\n",
    "                    [0.0, 1.0]], dtype=torch.float32)\n",
    "C = torch.tensor([[1.0, 0.1],\n",
    "                    [0.1, 1.0]], dtype=torch.float32)\n",
    "D = torch.tensor([[1.0, 0.0],\n",
    "                    [0.0, 1.0]], dtype=torch.float32)\n",
    "R = torch.tensor([[1.0, 0.3],\n",
    "                    [0.3, 1.0]], dtype=torch.float32) * 10.0\n",
    "sigma = torch.eye(2, dtype=torch.float32) * 0.5\n",
    "\n",
    "# Set final time and discretization for SoftLQR.\n",
    "T = 0.5\n",
    "N_points = 100\n",
    "time_grid = torch.linspace(0, T, N_points)\n",
    "\n",
    "# Build SoftLQR and solve the Riccati ODE.\n",
    "soft_lqr = SoftLQR(H, M, C, D, R, sigma, time_grid)\n",
    "soft_lqr.solve_ricatti_finite(T_max=T, tau=0.5, gamma=1.0)\n",
    "\n",
    "# Build the environment.\n",
    "initial_distribution = torch.FloatTensor(2).uniform_(-2.0, 2.0)\n",
    "env = LQREnvironmentWithPolicy(H, M, C, D, R, sigma, gamma=1.0,\n",
    "                                initial_distribution=initial_distribution,\n",
    "                                T=T, dt=0.005)\n",
    "\n",
    "# Train the actor-critic algorithm.\n",
    "actor_net, critic_net = actor_critic_algorithm(env, num_episodes=501, dt=0.005, tau=0.5, actor_lr=1e-6, critic_lr=1e-6)\n",
    "print(\"Actor-Critic training completed.\")\n",
    "\n",
    "# Compare learned value function (critic) with the optimal value function.\n",
    "compare_value_functions(critic_net, soft_lqr, times=[0.0, 1/6, 2/6, 0.5], x_vals=[-2.0, 0.0, 2.0])\n",
    "\n",
    "# Compare learned control distribution (actor) with the optimal control distribution.\n",
    "compare_control_distributions(actor_net, soft_lqr, times=[0.0, 1/6, 2/6, 0.5], x_vals=[-2.0, 0.0, 2.0], tau=0.5, gamma=1.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
